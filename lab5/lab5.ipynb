{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 5:\n",
    "    1. Download Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\n",
    "    2. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.\n",
    "    3. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?\n",
    "    4. Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?\n",
    "    5. *(not necessary) Find Top 100 most used verbs in sentences with Alice. Get word vectors using a pre-trained word2vec model and visualize them. Compare the words using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import urllib.request\n",
    "import spacy\n",
    "\n",
    "alice_handler = urllib.request.urlopen(\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "alice_text = alice_handler.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define english vocab\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Removing copyright, tokenize, lemmatize, stop words\n",
    "\n",
    "end = \"*** END OF THIS PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\"\n",
    "\n",
    "alice_text = alice_text[alice_text.index(\"CHAPTER I.\\r\"):]\n",
    "alice_text = alice_text[:alice_text.index(end) - len(end)]\n",
    "alice_text = alice_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alice_text_nlp = nlp(alice_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_text(doc):\n",
    "    Nouns = []\n",
    "    Noun_set = []\n",
    "    trimmed_noun_set = []\n",
    "    removing_duplicates = []\n",
    "    arr = []\n",
    "    vocab = []\n",
    "    vocab_dict = {}\n",
    "\n",
    "    nlp.vocab[\"alice\"].is_stop = True\n",
    "\n",
    "    doc = nlp(doc.lower())\n",
    "\n",
    "    for possible_nouns in doc:\n",
    "        if possible_nouns.pos_ in [\"NOUN\",\"PROPN\"] and not possible_nouns.is_stop:\n",
    "            Nouns.append(possible_nouns)\n",
    "\n",
    "\n",
    "    for i in Nouns:\n",
    "        Noun_set.append([i])\n",
    "\n",
    "    for i in Noun_set:\n",
    "            trimmed_noun_set.append([i])\n",
    "\n",
    "    for word in trimmed_noun_set:\n",
    "        if word not in removing_duplicates:\n",
    "            removing_duplicates.append(word)\n",
    "\n",
    "    for word in Noun_set:\n",
    "        string = ''\n",
    "        for j in word:\n",
    "            string+= str(j)+ \" \"\n",
    "        vocab.append(string.strip())\n",
    "\n",
    "    for word in vocab:\n",
    "        if word == \"_\":\n",
    "            continue\n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word]= 0\n",
    "        else:\n",
    "            vocab_dict[word]+=1\n",
    "    arr = vocab_dict.keys()\n",
    "    return vocab_dict , arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computeTF(wordDict,bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "def computeIDF(doclist):\n",
    "    count = 0\n",
    "    idfDict = {}\n",
    "    for element in doclist:\n",
    "        for j in element:\n",
    "            count+=1\n",
    "    N = count\n",
    "\n",
    "    # count number of usages of word w in doc\n",
    "    idfDict = dict.fromkeys(doclist[0].keys(),0)\n",
    "\n",
    "    for doc in doclist:\n",
    "        for word,val in doc.items():\n",
    "            if val>0:\n",
    "                idfDict[word]+= 1\n",
    "\n",
    "    # divide N by denominator above\n",
    "    for word,val in idfDict.items():\n",
    "        if val == 0:\n",
    "            idfDict[word] = 0.0\n",
    "        else:\n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "\n",
    "    return idfDict\n",
    "\n",
    "def computeTfidf(tf,idf):\n",
    "    tfidf = {}\n",
    "    sorted_list = []\n",
    "    for word , val in tf.items():\n",
    "        tfidf[word] = val * idf[word]\n",
    "\n",
    "    ranking_list  = sorted(tfidf.items(),reverse=True, key = lambda kv:(kv[1], kv[0]))[:10]\n",
    "    for i, _ in ranking_list:\n",
    "        sorted_list.append(i)\n",
    "\n",
    "    return sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split text into chapters\n",
    "import re\n",
    "chapters = re.split('chapter *', alice_text)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Find most import words using tf-idf metric. We will use pairs(pronoun+noun, verb+noun etc..) in order to come up with meaningfull chapter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words per chapter (a.k.a. chapter names):\n",
      "CHAPTER 1: way time rabbit door key use things table people moment\n",
      "CHAPTER 2: mouse way pool feet things cats tears time voice thing\n",
      "CHAPTER 3: mouse dodo thing race lory course question prizes birds party\n",
      "CHAPTER 4: rabbit bill window thing voice room puppy moment gloves door\n",
      "CHAPTER 5: caterpillar pigeon youth size bit tone time serpent mouth father\n",
      "CHAPTER 6: cat footman duchess baby way pig door cook thing tone\n",
      "CHAPTER 7: hatter dormouse march hare time tea thing twinkle treacle table\n",
      "CHAPTER 8: queen king head cat soldiers gardeners game voice rabbit minute\n",
      "CHAPTER 9: turtle gryphon duchess queen day tone thing school moral course\n",
      "CHAPTER 10: gryphon turtle dance soup voice whiting sea soo oop lobsters\n",
      "CHAPTER 11: king hatter court dormouse witness queen rabbit jury voice march\n",
      "CHAPTER 12: king jury queen rabbit sister head voice dream verses time\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(f\"Top 10 words per chapter (a.k.a. chapter names):\")\n",
    "for i,chapter in enumerate(chapters):\n",
    "    vocab_dict , arr = prepare_text(chapter)\n",
    "    tf = computeTF(vocab_dict,arr)\n",
    "    idf = computeIDF([vocab_dict])\n",
    "    tfidf = computeTfidf(tf,idf)\n",
    "    chapter_name = \" \".join(tfidf)\n",
    "    \n",
    "    print(f\"CHAPTER {i+1}: {chapter_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ignore verbs which probably refer to Alice since there is really a lot of them in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_alice_verbs(doc):\n",
    "    verbs = []\n",
    "\n",
    "    doc = nlp(doc.upper())\n",
    "\n",
    "\n",
    "    for possible_nouns in doc:\n",
    "        #print(possible_nouns.lemma_)\n",
    "        if possible_nouns.lemma_.lower()==\"alice\":\n",
    "            if possible_nouns.head.pos_ == \"VERB\" and possible_nouns.head.text.lower() != 'alice':\n",
    "\n",
    "                cand = [possible_nouns.text, possible_nouns.head.text]\n",
    "                verbs.append(cand[1])\n",
    "    return set(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_count_verbs = {}\n",
    "for chapter in chapters:\n",
    "    cur_keys = find_alice_verbs(chapter)\n",
    "\n",
    "    for key in cur_keys:\n",
    "        if key in total_count_verbs:\n",
    "            total_count_verbs[key]+=1\n",
    "        else:\n",
    "            total_count_verbs[key]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SAID': 12,\n",
       " 'THOUGHT': 6,\n",
       " 'WENT': 5,\n",
       " 'BEGAN': 5,\n",
       " 'HAD': 3,\n",
       " 'THINK': 3,\n",
       " 'BE': 3,\n",
       " 'BEEN': 3,\n",
       " 'CRIED': 3,\n",
       " 'TURNING': 3,\n",
       " 'WAITED': 3,\n",
       " 'WAS': 3,\n",
       " 'SEE': 3,\n",
       " 'VENTURED': 2,\n",
       " 'HEARD': 2,\n",
       " 'LOOKING': 2,\n",
       " 'SEEN': 2,\n",
       " 'BEGINNING': 2,\n",
       " 'TELL': 2,\n",
       " '’S': 2,\n",
       " 'LIKE': 2,\n",
       " 'MOVED': 2,\n",
       " 'GOT': 2,\n",
       " 'LOOKED': 2,\n",
       " 'SHUTTING': 1,\n",
       " 'ADVISE': 1,\n",
       " 'STARTED': 1,\n",
       " 'TOOK': 1,\n",
       " 'UNDERSTAND': 1,\n",
       " 'OUGHT': 1,\n",
       " 'KEPT': 1,\n",
       " 'CALLED': 1,\n",
       " 'ALLOW': 1,\n",
       " 'POINTING': 1,\n",
       " 'SEEMED': 1,\n",
       " 'WHISPERS': 1,\n",
       " 'HAVE': 1,\n",
       " 'CROUCHED': 1,\n",
       " 'CAN’T': 1,\n",
       " 'DID': 1,\n",
       " 'SAW': 1,\n",
       " 'KEEP': 1,\n",
       " 'TRIED': 1,\n",
       " 'KNOW': 1,\n",
       " 'COMING': 1,\n",
       " 'WISH': 1,\n",
       " 'CONSIDERED': 1,\n",
       " 'SUPPOSE': 1,\n",
       " 'SHOUTED': 1,\n",
       " 'GAVE': 1,\n",
       " 'CAME': 1,\n",
       " 'FOUND': 1,\n",
       " 'WANTED': 1,\n",
       " 'SPEAK': 1,\n",
       " 'HEAR': 1,\n",
       " 'FEEL': 1,\n",
       " 'LEAVING': 1,\n",
       " 'TAKING': 1,\n",
       " 'DARE': 1,\n",
       " 'THANK': 1,\n",
       " 'ASKED': 1,\n",
       " 'PANTED': 1,\n",
       " 'IMAGINE': 1,\n",
       " 'MADE': 1,\n",
       " 'WAKE': 1,\n",
       " 'LET': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most used verbs\n",
    "dict(sorted(total_count_verbs.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
