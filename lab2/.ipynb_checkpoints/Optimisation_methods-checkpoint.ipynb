{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwuLDB1F_eCG"
   },
   "source": [
    "# Methods of optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69w_hqdw_eCK"
   },
   "source": [
    "There is __no single method__ available for solving all optimization problems efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjtplmV6_eCL"
   },
   "source": [
    "One can use a number of different methods depending on the specific problem. \n",
    "\n",
    "- __Mathematical programming methods.__ These are useful in finding the minimum of a function of several variables under a prescribed set of constraints.\n",
    "- __Stochastic process techniques.__ These are used to analyze problems which are described by a set of random variables of known distribution.\n",
    "- __Statistical methods.__These are used in the analysis of experimental data and in the construction of empirical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSPURL6f_eCM"
   },
   "source": [
    "## Simple examples with SciPy: 1D optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXN2Ll7x_eCN"
   },
   "source": [
    "__Non-conditional oprimization.__ Objective function to be minimized: $$min f(x^2 - 2x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAczrmOl_eCO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZOQS4Ed_eCS"
   },
   "outputs": [],
   "source": [
    "#set objective function\n",
    "objective = np.poly1d([1.0, -2.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiC-o_xP_eCV"
   },
   "outputs": [],
   "source": [
    "# initialize first value\n",
    "x0 = 3.0\n",
    "results = opt.minimize(objective,x0) \n",
    "print(\"Solution: x=%f\" % results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV_elJx2_eCZ"
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "x = np.linspace(-3,5,100)\n",
    "plt.plot(x,objective(x))\n",
    "plt.plot(results.x,objective(results.x),'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMK0VTDz_eCc"
   },
   "source": [
    "__With condition.__ Objective function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UelEa_i8_eCd"
   },
   "source": [
    "$$ min f(x^2 - 2x)$$\n",
    "$$x - 2 >= 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11fKhCMO_eCe"
   },
   "outputs": [],
   "source": [
    "#set objective and conditions\n",
    "objective = np.poly1d([1.0, -2.0, 0.0])\n",
    "cons = ({'type': 'ineq','fun' : lambda x: np.array([x[0] - 2])}) \n",
    "\n",
    "results = opt.minimize(objective,x0=3.0,\n",
    "                       constraints = cons,\n",
    "                       options = {'disp':True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OWnvs0G_eCh"
   },
   "outputs": [],
   "source": [
    "#plot results\n",
    "x = np.linspace(-3,5,100)\n",
    "cond = np.poly1d([1.0, -2.0])\n",
    "\n",
    "plt.plot(x,objective(x))\n",
    "plt.plot(x, cond(x))\n",
    "plt.plot(results.x,objective(results.x),'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BEKbcJV_eCk"
   },
   "source": [
    "## Newton's methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQN9trjV_eCl"
   },
   "source": [
    "Newton's method is a root finding method that uses linear approximation. \n",
    "We guess a solution of $x_0$ the equation $f(x) =0 $, compute the linear approximation of $f(x)$ at $x_0$ and then find the $x$-intercept of the linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7tt6867_eCn"
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_bGi-H-_eCn"
   },
   "outputs": [],
   "source": [
    "def newton(f,Df,x0,epsilon,max_iter):\n",
    "    '''Approximate solution of f(x)=0 by Newton's method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        Function for which we are searching for a solution f(x)=0.\n",
    "    Df : function\n",
    "        Derivative of f(x).\n",
    "    x0 : number\n",
    "        Initial guess for a solution f(x)=0.\n",
    "    epsilon : number\n",
    "        Stopping criteria is abs(f(x)) < epsilon.\n",
    "    max_iter : integer\n",
    "        Maximum number of iterations of Newton's method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xn : number\n",
    "        Implement Newton's method: compute the linear approximation\n",
    "        of f(x) at xn and find x intercept by the formula\n",
    "            x = xn - f(xn)/Df(xn)\n",
    "        Continue until abs(f(xn)) < epsilon and return xn.\n",
    "        If Df(xn) == 0, return None. If the number of iterations\n",
    "        exceeds max_iter, then return None. '''\n",
    "\n",
    "   \n",
    "    xn = x0\n",
    "    for n in range(0,max_iter):\n",
    "        fxn = f(xn)\n",
    "        if abs(fxn) < epsilon:\n",
    "            print('Found solution after',n,'iterations.')\n",
    "            return xn\n",
    "        Dfxn = Df(xn)\n",
    "        if Dfxn == 0:\n",
    "            print('Zero derivative. No solution found.')\n",
    "            return None\n",
    "        xn = xn - fxn/Dfxn\n",
    "    print('Exceeded maximum iterations. No solution found.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySg1F7xb_eCq"
   },
   "outputs": [],
   "source": [
    "#set function and first derivative\n",
    "f = lambda x: x**3 - x**2 - 1\n",
    "Df = lambda x: 3*x**2 - 2*x\n",
    "\n",
    "#apply optimization methods\n",
    "approx = newton(f = f, Df = Df, x0 = 1, epsilon = 1e-10, max_iter = 10)\n",
    "print(approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XdrXujL_eCt"
   },
   "outputs": [],
   "source": [
    "#plot results \n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "poly = np.poly1d([1.0, -1.0, 0, -1])\n",
    "\n",
    "plt.plot(x,poly(x))\n",
    "plt.plot(approx,poly(approx),'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BuftJHz_eCw"
   },
   "source": [
    "The __scipy.optimize__ package provides several commonly used optimization algorithms. This module contains the following aspects:\n",
    "- __Unconstrained and constrained minimization of multivariate scalar functions__ (minimize()) using a variety of algorithms (e.g. BFGS, Nelder-Mead simplex, Newton Conjugate Gradient, COBYLA or SLSQP)\n",
    "- Global (brute-force) optimization routines (e.g., anneal() (Metropolis alg), basinhopping())\n",
    "- Least-squares minimization (leastsq()) and curve fitting (curve_fit()) algorithms\n",
    "- Scalar univariate functions minimizers (minimize_scalar()) and root finders (newton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnXGROwC_eCw"
   },
   "source": [
    "Let's apply Newton's methods with ScyPy for minimization of Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUL7sXsn_eCx"
   },
   "source": [
    "The __Rosenbrock function__ is a non-convex function, introduced by Howard H. Rosenbrock in 1960, which is used as a performance test problem for optimization algorithms. It is also known as Rosenbrock's valley or Rosenbrock's banana function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa3kSz7i_eCy"
   },
   "source": [
    "__Rosenbrock function in SciPy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmlpuflW_eCz"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen\n",
    "\n",
    "#or \n",
    "\n",
    "def rosen(x):\n",
    "    return sum(100*(x[1:]-x[:-1]**2.0)**2.0 +(1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN5dbo48_eC2"
   },
   "source": [
    "__Plot Rosenbork function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0wFfu5C_eC3"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "# 3D plot\n",
    "fig = plt.figure(figsize=[15, 10])\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# ang of view\n",
    "ax.view_init(45, 30)\n",
    "\n",
    "#generate data for plot\n",
    "X = np.arange(-2, 2, 0.1)\n",
    "Y = np.arange(-1, 3, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = rosen(np.array([X,Y]))\n",
    "\n",
    "# plot data\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xteBAsu_eDB"
   },
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmWlKr3D_eDC"
   },
   "source": [
    "Boyden–Fletcher–Goldfarb–Shanno algorithm [(more)](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKrtBn7z_eDC"
   },
   "outputs": [],
   "source": [
    "ps = [x0]\n",
    "opt.minimize(rosen, x0, method='BFGS', callback=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXgFX5b0_eDF"
   },
   "outputs": [],
   "source": [
    "ps = np.array(ps)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 40, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosen(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.contour(X, Y, Z, np.arange(10)**5)\n",
    "plt.plot(ps[:, 0], ps[:, 1], '-o')\n",
    "plt.subplot(122)\n",
    "plt.semilogy(range(len(ps)), rosen(ps.T));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_wWBjB__eDI"
   },
   "source": [
    "### Nelder-mead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obUc9RsA_eDJ"
   },
   "source": [
    "__Nealder-Mead__ algorithm [(more)](https://en.wikipedia.org/wiki/Nelder–Mead_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6CFVSji_eDK"
   },
   "outputs": [],
   "source": [
    "ps = [x0]\n",
    "opt.minimize(rosen, x0, method='nelder-mead', callback=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibsrgxay_eDN"
   },
   "outputs": [],
   "source": [
    "ps = np.array(ps)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-7, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosen(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.contour(X, Y, Z, np.arange(10)**5)\n",
    "plt.plot(ps[:, 0], ps[:, 1], '-o')\n",
    "plt.subplot(122)\n",
    "plt.semilogy(range(len(ps)), rosen(ps.T));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4voroRUo_eDP"
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDbB85yA_eDQ"
   },
   "source": [
    "Gradient descent is an algorithm used to the find local minimums of any differentiable function. \n",
    "\n",
    "More formally, given a differentiable function $f(x),$ the gradient descent algorithms helps us compute $x^*$ such that $f'(x^*) = 0$ and $x^*$ is a minimum of $f(x).$ A function can have many local minimums $x_{1}^{*}, x_{2}^{*}, \\ldots, x_{k}^{*},$ the gradient descent algorithm will converge to one of them $\\textbf{depending on the its starting position and learning rate.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM8oNFUT_eDR"
   },
   "source": [
    "__Single variable__\n",
    "\n",
    "Suppose we have a single variable and differentiable function $f(x).$ Given a initial value $x_{1}$ the gradient descent describes the steps required to coverge towards a local minimum $x^*$ such that $f'(x^*) = 0.$ In the above section we determined that $x_{2} = x_{1} - \\lambda f'(x_{1}),$ where $\\lambda$ is the learning rate parameter. This formula can be generalized to $$x_{t+1} = x_{t} - \\lambda f'(x_{t}),$$ for itteration $t.$ Given initial value $x_{1}$ and large number of itterations $T,$ this algorithm will generate $x_{1}, x_{2}, \\ldots, x_{T},$ where $x_{T} \\approx x^*.$ That is $x_{t+1}$ converges towards $x^*$ as $t$ gets very large. Notice that at convergence $f'(x_{t}) \\approx 0$ and hence $|x_{t+1} - x_{t}| \\approx 0.$ Therefore a common stopping criteria for gradience descent is to itterate until $|x_{t+1} - x_{t}|$ is a very small number. For example, we can keep iterating gradient descent until $|x_{t+1} - x_{t}|<0.001.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk9Tvgcr_eDR"
   },
   "source": [
    "__Multiple variables__\n",
    "\n",
    "Consider a multiple variable and differentiable function $f(x_{1},x_{2},\\ldots, x_{n}).$ Our goal is to apply gradient descent and find a minimum $(x_{1}^{*}, x_{2}^{*}, \\ldots, x_{n}^{*}).$ This function has partial derivatives stored in the gradient vector $(\\frac{df(x)}{dx_{1}}, \\ldots, \\frac{df(x)}{dx_{n}}).$ The direction of the gradient vector indicates direction of steepest ascent, and the length of the gradient vector is a mesuare of the steepness. We can easily generalize the gradient descent to multiple variables as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x_{1}^{t+1} \\\\ \n",
    "    \\vdots \\\\\n",
    "    x_{n}^{t+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    x_{1}^{t} \\\\ \n",
    "    \\vdots \\\\\n",
    "    x_{n}^{t}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "    \\frac{df(x_{1})}{dx_{1,t}} \\\\ \n",
    "    \\vdots \\\\\n",
    "    \\frac{df(x_{n})}{dx_{n,t}} \\\\ \n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $t$ is the gradient descent itteration. Now the stopping criteria can depend on the euclidean distance between the now and previous itteration, that is stop if $\\sqrt{(x_{1}^{t+1} - x_{1}^{t})^2 + \\ldots + (x_{n}^{t+1} - x_{n}^{t})^2}<0.001.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2YwN4J7_eDS"
   },
   "source": [
    "### SImple example: minimizing single variable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9icUW7Y_eDT"
   },
   "source": [
    "__Objective function__: $$f(x) = 0.1x^2 + sin(0.1x^2)$$ \n",
    "\n",
    "Gradient descent requires us to compute the __first derivative__ which is:\n",
    "\n",
    "$$f'(x) = 0.2x + 0.2xcos(0.1x^2).$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNL4f7jK_eDT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# objective function\n",
    "def f(x):\n",
    "    return 0.1*x**2 + np.sin(0.1*x**2)\n",
    "\n",
    "def plotf(x, xdots):\n",
    "    y = f(x)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.plot(xdots, f(xdots), 'bo')\n",
    "    plt.show()\n",
    "    \n",
    "x = np.arange(-10, 10, 0.1)\n",
    "plotf(x, np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHIjDP4Y_eDW"
   },
   "outputs": [],
   "source": [
    "#first derivative\n",
    "def df(x):\n",
    "    return 0.2*x + 0.2*x*np.cos(0.1*(x**2))\n",
    "\n",
    "# gradient descent for single var\n",
    "def grad_descent(df, x_prev, learn_rate, sequence):\n",
    "    \n",
    "    '''Approximate solution of f(x)=0 by Gradient Descent method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : function\n",
    "        First derivative of f(x).\n",
    "    x_prev : number\n",
    "        Initial guess for a solution f(x)=0.\n",
    "    learn_rate: number\n",
    "        Step size of gradient descent.\n",
    "    max_iter : integer\n",
    "        Maximum number of iterations .\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_next : number\n",
    "        The minimal value of function.\n",
    "    sequence: array\n",
    "        Intermidiate results of gradient descent\n",
    "    grad_itter: number\n",
    "        Number of iterations. '''\n",
    "    \n",
    "    # epsilon threshold - stopping criteria\n",
    "    epsilon = 0.0001\n",
    "    \n",
    "    # itterations number\n",
    "    grad_itter = 1\n",
    "    \n",
    "    # vey first itteration of gradient descent\n",
    "    x_next = x_prev - learn_rate*df(x_prev)\n",
    "    sequence = np.append(sequence, x_next)\n",
    "\n",
    "    # while abs(x_{t+1} - x_{t}) > threshold, keep itterating gradient descent\n",
    "    while abs(x_next - x_prev) > epsilon:\n",
    "        x_prev = x_next\n",
    "        x_next = x_prev - learn_rate*df(x_prev)\n",
    "        sequence = np.append(sequence, x_next)\n",
    "        grad_itter += 1\n",
    "        \n",
    "    return (x_next, sequence, grad_itter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpc_s_hS_eDY"
   },
   "outputs": [],
   "source": [
    "# apply gradient descent to objective function\n",
    "grad_output = grad_descent(df = df, x_prev = 8, learn_rate = 1, sequence = np.array([8]))\n",
    "\n",
    "plotf(x, grad_output[1])\n",
    "\n",
    "# Print gradient descent solution\n",
    "print('Minimum x-value from gradient descent: %s' %grad_output[0])\n",
    "print('Number of gradient descent iterations: %s' %grad_output[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVbtMD8N_eDa"
   },
   "source": [
    "### Example for multiple variable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqTs2IzD_eDb"
   },
   "source": [
    "__Objective function__:\n",
    "$$f(x,y) = x^2 + y^2 + 1.$$ \n",
    "\n",
    "__Partial derivatives__:\n",
    "\n",
    "$$\\frac{df(x)}{dx} = f_{x} = 2x \\text{  and  } \\frac{df(y)}{dy} = f_{y} = 2y.$$ \n",
    "\n",
    "\n",
    "__Global minimum__ of $f(x,y)$ is (0,0) since $f(0,0) = 1$ and $f(x,y) \\ge 1.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82PPtj8W_eDc"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "#set objective function\n",
    "def f(x,y):\n",
    "    return x**2 + y**2 + 1\n",
    "\n",
    "#plot function\n",
    "y = np.arange(-5, 5, 0.25)\n",
    "y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "fig = plt.figure(1, figsize = (15, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, \n",
    "                      cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "fig.colorbar(surf, shrink = 0.7, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDWZLXsA_eDe"
   },
   "outputs": [],
   "source": [
    "def dfdx(x,y):\n",
    "    return 2*x\n",
    "\n",
    "def dfdy(x,y):\n",
    "    return 2*y\n",
    "\n",
    "# gradient vector [df/dx, df/dy]\n",
    "def grad_vector(x,y):\n",
    "    return np.array([dfdx(x,y), dfdy(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SijbuYi_eDh"
   },
   "outputs": [],
   "source": [
    "def grad_descent_mult(df,x_prev,learn_rate):\n",
    "    \n",
    "    epsilon = 0.0001\n",
    "    grad_itter = 1\n",
    "    \n",
    "    seq_x = []\n",
    "    seq_y = []\n",
    "\n",
    "    x_next = np.subtract(x_prev, learn_rate*grad_vector(x_prev[0],x_prev[1]))\n",
    "    seq_x = np.append(seq_x, x_next[0])\n",
    "    seq_y = np.append(seq_y, x_next[1])\n",
    "    \n",
    "    while np.linalg.norm(np.subtract(x_next,x_prev)) > epsilon:\n",
    "        \n",
    "        x_prev = x_next\n",
    "        x_next = x_prev - learn_rate*grad_vector(x_prev[0],x_prev[1])\n",
    "        grad_itter += 1\n",
    "        \n",
    "        seq_x = np.append(seq_x, x_next[0])\n",
    "        seq_y = np.append(seq_y, x_next[1])\n",
    "    \n",
    "    return (x_next, grad_itter, seq_x,seq_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex8ryWDE_eDj"
   },
   "outputs": [],
   "source": [
    "def contour_plot(x_seq,y_seq, x1):\n",
    "    plt.figure(1)\n",
    "    CS = plt.contour(X, Y, Z, cmap=cm.RdBu)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.plot(x1[0],x1[1],'ro')\n",
    "    \n",
    "    plt.plot(x_seq,y_seq,'ro')\n",
    "    plt.show()\n",
    "\n",
    "grad_output = grad_descent_mult(grad_vector, np.array([6,2]), 0.2)\n",
    "\n",
    "print('Minimum (x,y) from gradient descent: %s' %grad_output[0])\n",
    "print('Number of gradient descent iterations: %s' %grad_output[1])\n",
    "\n",
    "contour_plot(grad_output[2],grad_output[3], np.array([6,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s8KJQ9o_eDm"
   },
   "source": [
    "## Specific gradient descents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBh2e_t9_eDn"
   },
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mp2LEQn_eDn"
   },
   "source": [
    "Gradient descent algorithm may be infeasible when the training data size is very large.\n",
    "\n",
    "$\\textbf{Classic}$ gradient descent step to update parameters of the model:\n",
    "\n",
    "$$ \\textbf{$\\theta_t$} := \\textbf{$\\theta_{t-1}$} - \\eta \\textbf{∇}_\\theta J(\\textbf{$\\theta_{t-1}$})_,$$\n",
    "\n",
    "where $\\eta$ - learning rate;\n",
    "$\\theta$ - model's parameters;\n",
    "\n",
    "We need to look through the full set of samples for gradient $\\textbf{∇}_\\theta J(\\textbf{$\\theta_{t-1}$})$ evaluation to obtain parameters $\\theta$.\n",
    "In a cases when data is very large it will be computationally expensive operation.\n",
    "Thus, a __stochastic__ version of the algorithm is often used instead.\n",
    "\n",
    "Stochastic gradient descent (SGD) in contrast performs a parameters (weights) update for $\\textbf{each}$ training example $(i)$ and label $y^{(i)}$:\n",
    "\n",
    "$$ \\textbf{$\\theta_{t}$} := \\textbf{$\\theta_{t-1}$} - \\eta ∇_\\theta J(\\theta^{(i)}_{t-1}),$$\n",
    "\n",
    "wher $J(\\theta)$ - objective (loss) function, $\\theta$ - model's parameters, $\\eta$ - learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu5l5I-D_eDo"
   },
   "source": [
    "### Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0SwFJIk_eDp"
   },
   "source": [
    "This is a method that helps accelerate GD in the relevant direction and suppresses oscillations. It implies adding a fraction $\\beta$ of the previous vector adding the fraction $(1 - \\beta)$ of the upcoming gradient. \n",
    "This approach is called \"exponential moving average\":\n",
    "\n",
    "$$V_t = \\beta V_{t-1} + (1 - \\beta) g_t$$\n",
    "$$ \\textbf{$\\theta_{t}$} := \\textbf{$\\theta_{t-1}$} - \\eta V_t,$$\n",
    "\n",
    "where $g_t$ - is the current gradient.\n",
    "\n",
    "$\\beta$ is usually set to 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoHUGNwE_eDp"
   },
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGiyaSOo_eDr"
   },
   "source": [
    "Root Mean Square propagation algorithm updates paramemters accordingly to the square of the upcoming gradients and adapts learning rate:\n",
    "\n",
    "$$V_t = \\beta V_{t-1} + (1 - \\beta) g_t^2$$\n",
    "\n",
    "$$ \\textbf{$\\theta_{t}$} := \\textbf{$\\theta_{t-1}$} - \\eta \\frac{g_t}{\\sqrt{V_t} + \\epsilon}$$\n",
    "\n",
    "Adaptive learning rate with the square root in the last formula provides faster learning in the desirable direction to the local minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8V-2dQ3_eDt"
   },
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z0AduyE_eDu"
   },
   "source": [
    "Adam  (Adaptive momentum algorithm) combines momentum and RMSprop approaches:\n",
    "\n",
    "- computes adaptive learning rates;\n",
    "- storing an exponentially moving average of past squared gradients\n",
    "\n",
    "and uses scaling of the gradients:\n",
    "\n",
    "$${V_t^{corr}}= \\frac{V_t}{1 - \\beta^t}$$\n",
    "\n",
    "Parameters:\n",
    "- learning rate $\\eta$\n",
    "- $\\beta_1$\n",
    "- $\\beta_2$\n",
    "- $\\epsilon$\n",
    "\n",
    "The moving averages of past and past squared gradients $V_t$ and $S_t$ respectively are computed as follows:\n",
    "\n",
    "$$V_t = \\beta_1 V_{t-1} + (1 - \\beta_1)g_t$$\n",
    "$$S_t = \\beta_2 S_{t-1} + (1 - \\beta_2)g^2_t$$\n",
    "\n",
    "Adam update rule for the model weights:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{{S_t^{corr}}} + \\epsilon}{V_t}$$\n",
    "\n",
    "Suggested values for $\\beta_1$ is 0.9, for $\\beta_2$ is 0.999 and $10^{-8}$ for $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujzKkSOQ_eDv"
   },
   "source": [
    "## Digits recognition example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9fT3NUk_eDv"
   },
   "source": [
    "### Step 1. Download and plot digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "gcaKN-iK_eDw",
    "outputId": "20a4e0ed-00c9-4b25-f2c8-4e1f210ef45c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAD7CAYAAAAVQzPHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ7DZEZET2OyIyAn/D0EV1fL8aMxGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using keras framework\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGFNMffk_eDz"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYO3DEjw_eD1"
   },
   "source": [
    "### Step 2. Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROieyscW_eD1"
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhaQP6Zk_eD4"
   },
   "source": [
    "### Step 3. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MZ7hDsd_eD4"
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxSsJUpg_eD9"
   },
   "source": [
    "### Step 4. Build and fit models with diff optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "FKKVP7af_eD-",
    "outputId": "75698445-6de7-40fa-fd7a-3fd953ca50c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 1.2511 - accuracy: 0.7020 - val_loss: 0.7119 - val_accuracy: 0.8440\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.6061 - accuracy: 0.8590 - val_loss: 0.4991 - val_accuracy: 0.8784\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.4765 - accuracy: 0.8789 - val_loss: 0.4214 - val_accuracy: 0.8926\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.4179 - accuracy: 0.8900 - val_loss: 0.3801 - val_accuracy: 0.9002\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3829 - accuracy: 0.8972 - val_loss: 0.3528 - val_accuracy: 0.9071\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3589 - accuracy: 0.9017 - val_loss: 0.3334 - val_accuracy: 0.9109\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3408 - accuracy: 0.9059 - val_loss: 0.3188 - val_accuracy: 0.9133\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3264 - accuracy: 0.9097 - val_loss: 0.3071 - val_accuracy: 0.9156\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3145 - accuracy: 0.9124 - val_loss: 0.2968 - val_accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3044 - accuracy: 0.9150 - val_loss: 0.2888 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr = 0.001, momentum = 0.9), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_SGD = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgYpbTiD_eEB"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.0001, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_Adam = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT0kOUDz_eED"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = 0.0001,epsilon=0.01), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_Adagrad = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9evORPD_eEF"
   },
   "source": [
    "### Step 5. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "dITUdqH1_eEG",
    "outputId": "db24bd8b-16df-42d2-fa13-efebeaff852e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAFNCAYAAACJ7U8aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npvr/5o9CzmVO5n0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizers = ['SGD',  'RMSprop','Adam']\n",
    "fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
    "for i, h in enumerate([history_SGD,  history_Adagrad,history_Adam]):\n",
    "    axs[i].set_ylim(0.1, 2.5)\n",
    "    axs[i].plot(h.history['val_loss'], label='val_loss')\n",
    "    axs[i].plot(h.history['loss'] , label='loss')\n",
    "    axs[i].set_xlabel(\"Epochs\")\n",
    "    axs[i].set_ylabel(\"Loss\")\n",
    "    axs[i].set_title(optimizers[i])\n",
    "    axs[i].legend(shadow=True, loc=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhGK0ZL4_eEI"
   },
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "E2YwN4J7_eDS",
    "IVbtMD8N_eDa"
   ],
   "name": "Optimisation_methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
